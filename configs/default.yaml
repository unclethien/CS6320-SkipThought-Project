# Configuration for Transformer-Based Latent GAN

seed: 42
output_dir: results/latent_gan_run

data:
  # path: "./data" # Path to dataset
  dataset_name: 'wikitext' # Use wikitext
  dataset_config_name: 'wikitext-103-raw-v1' # Specific config for wikitext-103
  filter_author: null # Specify author/series to filter BookCorpus if needed
  text_column: 'text' # Column name in the dataset containing text
  max_seq_length: 30 # Max sequence length for the Encoder/Decoder

model:
  # --- Text Autoencoder --- 
  encoder:
    # Options: 'bert-base-uncased', 'sentence-transformers/all-MiniLM-L6-v2', etc.
    # Or specify custom architecture params if not using pre-trained directly
    name: 'sentence-transformers/all-MiniLM-L6-v2'
    trainable: false # Freeze pre-trained encoder initially?
    # latent_dim will be determined by the chosen encoder (e.g., 384 for all-MiniLM-L6-v2)
  
  decoder:
    # Needs to match the latent_dim from the encoder
    # Example custom Transformer Decoder params (adjust as needed)
    latent_dim: 384 # Must match encoder output dim
    vocab_size: 30522 # Example: BertTokenizer vocab size (adjust based on chosen tokenizer)
    n_layer: 4
    n_head: 6         # Ensure latent_dim is divisible by n_head
    d_model: 384      # Match latent_dim
    d_ff: 1536        # Feed-forward dim (e.g., 4*d_model)
    dropout: 0.1
    # Add other necessary params (e.g., max_length matching data.max_seq_length)
    max_length: 30 

  # --- Latent Space GAN --- 
  gan_generator:
    noise_dim: 100      # Dimension of the input noise vector
    latent_dim: 384     # Output dimension (must match encoder/decoder)
    hidden_dims: [512, 512] # Hidden layer sizes for MLP generator

  gan_discriminator:
    latent_dim: 384     # Input dimension (must match encoder/decoder)
    hidden_dims: [512, 512] # Hidden layer sizes for MLP discriminator

training:
  batch_size: 64 # Adjust based on memory
  num_epochs: 25 # Set epochs to 20 for full training
  pretrain_ae_epochs: 5 # Warm-start autoencoder MLE epochs
  # Optimizers (Adam recommended for GANs)
  lr_g: 5e-5            # Learning rate for GAN Generator
  lr_d: 1e-4            # Learning rate for GAN Discriminator
  lr_ae: 1e-4           # Learning rate for Autoencoder (if trained jointly)
  betas_g: [0.5, 0.999]
  betas_d: [0.5, 0.999]
  betas_ae: [0.9, 0.999] # Different beta1 for AE often helps

  # WGAN-GP specific parameters
  n_critic: 5           # Number of discriminator updates per generator update
  lambda_gp: 10         # Gradient penalty coefficient

  # Loss weights (if combining losses)
  lambda_reconstruction: 1.0 # Weight for autoencoder reconstruction loss (if used)

  # Other params
  use_amp: true         # Use Mixed Precision
  device: auto
  seed: 42

# Evaluation settings (adjust as needed for generated text from decoder)
# Perplexity is no longer relevant here.
# We evaluate text generated by Decode(G(noise))
evaluation:
  eval_every_epochs: 1 # Evaluate on validation set every N epochs
  batch_size: 64 # Can be same or different from training
  num_samples_to_generate: 100
  generation_params: # Params for Decoder's generation (if needed)
    max_length: 30
    do_sample: true
    temperature: 0.7
    top_k: 50
    top_p: 0.9
  save_every_epochs: 1 # How often to save regular checkpoints
  best_metric: 'BLEU-4' # Metric used to determine the 'best' model checkpoint (e.g., BLEU-4, METEOR, ROUGE-L, Recon_Loss)
  # Metrics to compute (will compute all enabled ones)
  compute_bleu: true
  compute_meteor: true
