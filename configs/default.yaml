# Configuration for Transformer-Based Latent GAN

seed: 42
output_dir: results/latent_gan_run

data:
  # path: "./data" # Path to dataset
  dataset_name: 'bookcorpus' # Use bookcorpus
  filter_author: null # Specify author/series to filter BookCorpus if needed
  text_column: 'text' # Column name in the dataset containing text
  max_seq_length: 30 # Max sequence length for the Encoder/Decoder
  validation_split: 0.05 # Percentage of training data to use for validation
  test_split: 0.05       # Percentage of training data to use for testing
  subset_percentage: 0.1 # Percentage of TRAINING split to use (e.g., 0.1 for 10%), null for full dataset
  seed: 42               # Seed for reproducibility in splitting

model:
  # --- Text Autoencoder --- 
  encoder:
    # Options: 'bert-base-uncased', 'sentence-transformers/all-MiniLM-L6-v2', etc.
    # Or specify custom architecture params if not using pre-trained directly
    name: 'sentence-transformers/all-MiniLM-L6-v2'
    trainable: true # Freeze pre-trained encoder initially?
    # latent_dim will be determined by the chosen encoder (e.g., 384 for all-MiniLM-L6-v2)
  
  decoder:
    # Needs to match the latent_dim from the encoder
    # Example custom Transformer Decoder params (adjust as needed)
    latent_dim: 384 # Must match encoder output dim
    vocab_size: 30522 # Example: BertTokenizer vocab size (adjust based on chosen tokenizer)
    n_layer: 4
    n_head: 6         # Ensure latent_dim is divisible by n_head
    d_model: 384      # Match latent_dim
    d_ff: 1536        # Feed-forward dim (e.g., 4*d_model)
    dropout: 0.1
    # Add other necessary params (e.g., max_length matching data.max_seq_length)
    max_length: 30 

  # --- Latent Space GAN --- 
  gan_generator:
    noise_dim: 100      # Dimension of the input noise vector
    latent_dim: 384     # Output dimension (must match encoder/decoder)
    hidden_dims: [512, 512] # Hidden layer sizes for MLP generator

  gan_discriminator:
    latent_dim: 384     # Input dimension (must match encoder/decoder)
    hidden_dims: [512, 512] # Hidden layer sizes for MLP discriminator

training:
  batch_size: 16 # Adjust based on memory (Reduced from 32)
  num_epochs: 25 # Increased from 20 to add 5 more GAN epochs
  pretrain_ae_epochs: 15 # Warm-start autoencoder MLE epochs
  # Optimizers (Adam recommended for GANs)
  lr_g: 2.5e-5            # Halved from 5e-5
  lr_d: 2.5e-5            # Halved from 1e-4
  lr_ae: 2.5e-5           # Changed from 1e-5 back to 2.5e-5 per user request
  betas_g: [0.5, 0.999]
  betas_d: [0.5, 0.999]
  betas_ae: [0.9, 0.999] # Different beta1 for AE often helps

  # WGAN-GP specific parameters
  n_critic: 1           # Number of discriminator updates per generator update
  lambda_gp: 10         # Gradient penalty coefficient
  gradient_clip_val: 1.0 # Max norm for gradient clipping (Reduced for stability)

  # Loss weights (if combining losses)
  lambda_reconstruction: 1.0 # Weight for autoencoder reconstruction loss (if used)

  # Other params
  use_amp: true         # Use Mixed Precision
  device: auto
  seed: 42

# Evaluation settings (adjust as needed for generated text from decoder)
# Perplexity is no longer relevant here.
# We evaluate text generated by Decode(G(noise))
evaluation:
  eval_every_epochs: 1 # Evaluate on validation set every N epochs
  batch_size: 16 # Matching training batch size
  num_samples_to_generate: 50 # Reduced number of samples to save memory
  generation_params: # Params for Decoder's generation (if needed)
    max_length: 30
    do_sample: true
    temperature: 0.7
    top_k: 50
    top_p: 0.9
  save_every_epochs: 1 # How often to save regular checkpoints
  best_metric: 'BERTScore_F1' # Metric used to determine the 'best' model checkpoint (e.g., BLEU-4, METEOR, ROUGE-L, Recon_Loss)
  # Metrics to compute (will compute all enabled ones)
  compute_bleu: true
  compute_meteor: true
